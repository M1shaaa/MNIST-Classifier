import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')
data.head()
label	pixel0	pixel1	pixel2	pixel3	pixel4	pixel5	pixel6	pixel7	pixel8	...	pixel774	pixel775	pixel776	pixel777	pixel778	pixel779	pixel780	pixel781	pixel782	pixel783
0	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
1	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
2	1	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
3	4	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
4	0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
5 rows Ã— 785 columns

data = np.array(data)
m, n = data.shape
np.random.shuffle(data) # shuffle before splitting into dev and training sets

data_dev = data[0:1000].T
Y_dev = data_dev[0]
X_dev = data_dev[1:n]
X_dev = X_dev / 255.

data_train = data[1000:m].T
Y_train = data_train[0]
X_train = data_train[1:n]
X_train = X_train / 255.
_,m_train = X_train.shape
def init_params():
    W1 = np.random.rand(10, 784) - 0.5
    b1 = np.random.rand(10, 1) - 0.5
    W2 = np.random.rand(10, 10) - 0.5
    b2 = np.random.rand(10, 1) - 0.5
    return W1, b1, W2, b2

def ReLU(Z):
    return np.maximum(Z, 0)

def softmax(Z):
    A = np.exp(Z) / sum(np.exp(Z))
    return A
    
def forward_prop(W1, b1, W2, b2, X):
    Z1 = W1.dot(X) + b1
    A1 = ReLU(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def ReLU_deriv(Z):
    return Z > 0

def one_hot(Y):
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    return one_hot_Y

def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):
    one_hot_Y = one_hot(Y)
    dZ2 = A2 - one_hot_Y
    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2, axis=1).reshape(-1, 1)
    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X.T)
    db1 = 1 / m * np.sum(dZ1, axis=1).reshape(-1, 1)
    return dW1, db1, dW2, db2

def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
    W1 = W1 - alpha * dW1
    b1 = b1 - alpha * db1    
    W2 = W2 - alpha * dW2  
    b2 = b2 - alpha * db2    
    return W1, b1, W2, b2
def get_predictions(A2):
    return np.argmax(A2, 0)

def get_accuracy(predictions, Y):
    print(predictions, Y)
    return np.sum(predictions == Y) / Y.size

def gradient_descent(X, Y, alpha, iterations):
    W1, b1, W2, b2 = init_params()
    for i in range(iterations):
        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)
        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)
        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)
        if i % 10 == 0:
            print("Iteration: ", i)
            predictions = get_predictions(A2)
            print(get_accuracy(predictions, Y))
    return W1, b1, W2, b2
W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)
Iteration:  0
[8 5 8 ... 5 8 5] [5 5 4 ... 7 3 6]
0.08929268292682926
Iteration:  10
[3 1 8 ... 7 3 4] [5 5 4 ... 7 3 6]
0.18565853658536585
Iteration:  20
[3 1 8 ... 7 3 4] [5 5 4 ... 7 3 6]
0.30309756097560975
Iteration:  30
[3 3 8 ... 7 3 4] [5 5 4 ... 7 3 6]
0.4125853658536585
Iteration:  40
[3 3 8 ... 7 3 6] [5 5 4 ... 7 3 6]
0.5067073170731707
Iteration:  50
[3 3 8 ... 7 3 6] [5 5 4 ... 7 3 6]
0.5695365853658536
Iteration:  60
[3 3 8 ... 7 3 6] [5 5 4 ... 7 3 6]
0.6155853658536585
Iteration:  70
[3 3 6 ... 7 3 6] [5 5 4 ... 7 3 6]
0.6448536585365854
Iteration:  80
[5 3 6 ... 7 3 6] [5 5 4 ... 7 3 6]
0.6673170731707317
Iteration:  90
[5 3 6 ... 7 3 6] [5 5 4 ... 7 3 6]
0.685609756097561
Iteration:  100
[5 3 6 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7015121951219512
Iteration:  110
[5 3 6 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7134878048780487
Iteration:  120
[5 3 6 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7243658536585366
Iteration:  130
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7339512195121951
Iteration:  140
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7424878048780488
Iteration:  150
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.749780487804878
Iteration:  160
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7575609756097561
Iteration:  170
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7645365853658537
Iteration:  180
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7703414634146342
Iteration:  190
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7756341463414634
Iteration:  200
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7814146341463415
Iteration:  210
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.786219512195122
Iteration:  220
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7909512195121952
Iteration:  230
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.795609756097561
Iteration:  240
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.7995365853658537
Iteration:  250
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.803219512195122
Iteration:  260
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8067317073170732
Iteration:  270
[5 3 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.81
Iteration:  280
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8127560975609756
Iteration:  290
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8158292682926829
Iteration:  300
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8183658536585365
Iteration:  310
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8210731707317073
Iteration:  320
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8233658536585365
Iteration:  330
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.826
Iteration:  340
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8279512195121951
Iteration:  350
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8304390243902439
Iteration:  360
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8322682926829268
Iteration:  370
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8343658536585365
Iteration:  380
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8359024390243902
Iteration:  390
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8375609756097561
Iteration:  400
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8391951219512195
Iteration:  410
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8406585365853658
Iteration:  420
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8420975609756097
Iteration:  430
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8435121951219512
Iteration:  440
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8449268292682927
Iteration:  450
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8463170731707317
Iteration:  460
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8472926829268292
Iteration:  470
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8488048780487805
Iteration:  480
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8497560975609756
Iteration:  490
[5 5 4 ... 7 3 6] [5 5 4 ... 7 3 6]
0.8508780487804878
def make_predictions(X, W1, b1, W2, b2):
    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)
    predictions = get_predictions(A2)
    return predictions

def test_prediction(index, W1, b1, W2, b2):
    current_image = X_train[:, index, None]
    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)
    label = Y_train[index]
    print("Prediction: ", prediction)
    print("Label: ", label)
    
    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()
test_prediction(3, W1, b1, W2, b2)
test_prediction(7, W1, b1, W2, b2)
test_prediction(9, W1, b1, W2, b2)
test_prediction(2, W1, b1, W2, b2)
test_prediction(65, W1, b1, W2, b2)
Prediction:  [8]
Label:  8

Prediction:  [6]
Label:  6

Prediction:  [1]
Label:  1

Prediction:  [4]
Label:  4

Prediction:  [3]
Label:  5

dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)
get_accuracy(dev_predictions, Y_dev)
[3 8 1 9 3 1 0 1 2 2 0 7 3 8 7 1 8 1 4 6 6 7 5 9 4 7 5 1 8 5 7 5 6 3 4 6 4
 9 3 3 2 6 7 6 2 3 0 2 7 7 6 5 2 8 4 6 3 1 9 6 9 4 6 1 4 6 2 1 3 6 0 1 1 1
 1 9 3 5 5 2 2 2 1 1 4 7 8 2 9 1 0 0 0 5 8 1 6 8 9 8 6 4 1 6 5 9 6 9 8 5 3
 2 9 3 0 8 6 9 7 2 9 4 7 1 8 1 3 0 9 2 1 9 1 9 2 8 4 2 4 9 8 3 2 9 1 2 2 4
 4 3 6 8 7 3 6 8 6 8 5 8 1 2 5 9 9 5 4 2 4 4 1 6 2 2 9 1 4 9 6 5 4 0 5 3 3
 8 4 7 0 2 2 8 0 8 8 5 2 9 9 9 3 4 3 2 8 7 5 0 6 6 1 5 3 4 3 8 1 7 8 2 1 5
 9 3 4 9 7 2 9 4 2 4 4 7 2 3 1 8 3 8 1 7 9 0 2 5 0 7 2 5 2 3 8 5 2 7 8 0 9
 2 4 0 1 5 8 8 7 9 4 2 3 9 9 7 7 2 4 6 9 0 4 1 0 3 0 0 6 9 6 4 5 2 5 6 7 3
 7 3 6 2 2 7 4 0 3 9 2 1 5 1 0 6 4 0 4 1 1 3 9 2 6 1 2 3 4 1 2 3 5 9 6 0 8
 6 8 8 2 9 3 6 5 6 2 9 8 1 9 7 6 1 0 0 8 4 7 0 5 0 4 7 7 7 1 0 1 8 9 5 1 5
 0 3 3 1 6 2 7 7 0 4 8 1 6 5 7 2 6 5 1 2 1 3 8 5 8 7 6 7 4 8 2 8 8 2 7 8 1
 9 0 1 9 3 7 8 0 0 2 0 0 4 6 4 9 9 9 7 0 7 3 9 3 1 8 0 0 5 4 4 0 0 1 0 1 8
 5 0 1 3 5 5 3 1 5 7 3 3 5 0 5 9 2 2 2 5 2 5 0 2 5 9 4 9 1 3 6 6 6 6 8 7 5
 9 9 6 2 1 5 6 0 7 4 3 8 5 0 2 9 0 3 6 1 2 7 4 3 6 2 8 4 8 6 4 6 9 2 6 1 1
 4 3 1 3 3 0 0 8 3 8 8 5 5 2 8 6 9 6 1 3 5 1 7 6 0 0 4 8 6 2 5 6 0 8 7 7 7
 0 0 8 4 7 1 1 4 3 7 5 5 6 5 6 1 3 2 0 8 0 3 1 4 7 7 3 2 6 0 0 0 4 9 7 0 5
 7 2 3 2 7 2 1 0 3 0 6 8 2 7 8 4 0 4 6 3 2 2 6 8 4 1 8 0 7 0 0 8 2 0 0 3 3
 3 0 8 8 6 9 7 2 2 8 8 0 7 3 9 9 1 7 8 0 7 0 0 7 4 9 6 5 0 9 6 6 3 5 9 0 6
 4 6 0 9 9 9 4 8 8 7 8 7 4 7 5 5 7 4 0 6 7 5 6 2 6 1 0 8 7 0 3 7 6 9 7 9 9
 8 7 6 3 3 8 7 9 5 4 7 7 6 9 5 7 1 0 1 2 7 0 4 7 1 0 5 4 2 3 6 7 6 7 5 3 4
 7 6 4 4 3 9 6 5 2 4 6 0 0 2 8 4 9 1 7 8 7 3 3 8 0 8 4 1 9 1 5 4 8 8 7 6 2
 8 8 4 5 9 4 7 9 8 3 1 4 3 3 9 1 5 0 7 3 0 2 4 4 2 8 9 9 0 3 5 0 0 2 6 2 2
 4 6 0 7 5 9 5 7 8 1 2 7 3 7 4 8 2 6 1 1 7 0 7 1 2 7 1 2 1 4 4 2 4 5 0 7 9
 1 4 0 7 3 2 0 1 7 5 7 7 6 8 9 7 6 2 4 4 6 4 1 5 7 0 7 1 3 7 0 2 1 1 6 9 9
 9 2 8 4 3 1 8 7 7 1 8 2 0 8 9 3 8 5 6 7 4 1 4 9 0 9 7 7 6 4 1 1 3 3 1 5 5
 1 2 8 6 0 2 5 5 6 3 4 1 3 0 6 0 1 4 1 2 9 3 5 7 9 3 3 3 5 0 3 8 5 5 7 4 9
 8 2 4 4 9 0 4 9 2 4 4 3 8 3 1 4 6 9 7 7 3 8 9 9 4 0 5 7 9 5 1 8 8 2 7 5 1
 0] [3 8 1 9 3 1 0 1 2 2 0 7 3 6 2 1 8 1 4 6 6 5 5 4 4 7 8 1 8 5 3 8 6 3 4 6 4
 9 5 7 2 6 7 6 2 3 0 2 7 7 6 5 2 8 9 6 3 1 9 6 9 4 6 1 4 6 2 1 3 6 0 3 1 7
 1 9 9 0 5 1 2 2 8 1 4 7 8 2 7 1 0 0 0 5 8 8 6 5 9 8 6 6 1 6 5 9 6 9 0 5 3
 0 9 3 0 8 6 9 7 1 9 4 7 1 8 1 5 0 9 2 1 9 3 9 5 8 4 3 4 9 4 3 2 9 8 2 8 4
 4 3 5 8 7 3 6 8 6 2 5 8 1 2 5 9 9 9 4 2 4 9 1 6 2 2 9 1 4 9 6 5 4 0 5 5 3
 8 4 7 0 2 2 8 0 8 8 5 2 9 9 4 0 4 3 4 8 7 5 0 6 6 1 5 5 4 3 5 1 7 8 2 1 5
 9 3 8 9 7 2 9 4 2 4 4 7 5 3 1 8 3 8 1 7 9 0 2 5 0 7 7 5 2 3 8 5 2 7 8 0 9
 2 4 0 5 6 8 8 7 9 4 6 3 9 7 7 7 0 4 6 9 0 9 1 0 3 0 5 6 9 8 4 5 2 5 5 7 3
 7 3 6 2 2 7 4 0 3 9 2 1 5 1 0 6 4 0 4 1 1 3 9 3 6 1 2 3 4 1 2 3 8 9 6 0 5
 6 8 8 7 9 3 6 5 6 2 9 8 8 9 7 6 1 0 0 8 4 7 0 5 0 9 7 7 9 1 0 1 3 9 9 1 5
 0 3 3 1 5 2 7 7 0 9 8 1 4 5 5 2 6 8 1 2 1 3 5 5 8 2 6 7 4 8 2 8 8 2 7 8 1
 9 0 1 9 3 4 8 0 0 2 0 0 4 6 4 9 9 3 2 0 7 3 8 3 1 8 0 0 5 4 4 0 0 1 0 1 5
 5 0 1 3 5 5 3 1 5 7 3 3 5 0 5 9 2 2 2 5 2 8 0 2 3 9 4 9 6 3 6 6 6 5 8 9 5
 9 4 6 2 1 5 6 0 7 4 5 8 5 0 7 9 0 3 6 1 4 7 4 3 6 2 8 5 8 6 4 6 9 6 6 1 1
 4 3 7 5 3 0 0 2 3 8 8 5 6 2 9 6 9 6 1 3 5 1 7 5 0 0 4 8 6 2 5 6 2 8 7 7 7
 0 0 2 4 7 1 1 4 3 7 9 5 6 5 6 1 3 2 0 8 0 3 1 9 7 7 3 3 6 0 9 0 4 9 3 0 3
 7 2 3 2 7 2 1 0 3 0 6 8 2 7 8 0 0 4 6 3 2 2 8 8 4 1 8 0 7 0 0 8 2 0 0 3 3
 3 0 3 8 6 9 7 2 2 8 1 4 7 5 9 9 1 7 8 0 7 0 0 7 4 9 6 5 0 9 5 6 3 5 7 0 6
 4 5 0 9 9 4 4 5 8 7 8 7 9 7 8 5 7 4 0 6 7 4 6 2 6 1 5 8 7 0 3 7 6 9 7 9 9
 8 7 6 3 7 0 9 9 5 4 7 7 2 4 5 7 1 0 1 6 7 0 4 7 1 0 5 4 2 3 6 7 6 7 5 9 4
 7 6 4 4 3 7 6 9 2 4 6 0 0 2 1 4 9 1 7 8 7 3 3 8 0 8 4 1 9 1 8 4 8 8 7 6 2
 8 3 4 3 9 4 7 9 8 3 1 4 3 3 9 1 5 8 9 3 0 6 4 4 2 8 9 9 0 2 5 0 0 2 6 2 2
 4 6 0 7 8 9 5 7 8 1 2 7 3 9 4 7 2 6 1 1 9 0 7 1 2 7 1 2 1 4 9 5 9 0 0 7 9
 1 4 0 7 3 2 0 1 8 5 7 7 6 8 9 2 6 2 8 4 6 4 1 5 7 0 7 1 3 7 5 3 1 1 6 9 9
 9 2 5 4 5 1 8 7 7 1 5 2 0 0 9 3 8 5 6 7 4 1 4 9 0 9 9 7 2 4 1 1 3 3 1 5 5
 1 2 8 6 0 2 5 5 6 3 6 1 3 0 9 0 1 4 1 2 9 3 5 7 9 3 3 2 5 0 3 8 3 5 7 4 8
 9 2 4 4 9 0 9 9 7 4 4 3 8 3 1 4 5 9 7 7 3 8 9 3 4 0 8 3 9 5 1 8 8 2 7 5 1
 0]
0.839
